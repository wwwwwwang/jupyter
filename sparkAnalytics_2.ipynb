{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawblocks = sc.textFile(\"file:///var/lib/myspark/linkage/csvdata\")\n",
    "rawblocks.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749142"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawblocks.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val head = rawblocks.take(10)\n",
    "head.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"\n",
      "37671,78628,1,?,1,?,1,1,1,1,1,TRUE\n",
      "57064,74984,1,1,1,?,1,1,1,1,1,TRUE\n",
      "3149,8328,1,?,1,?,1,1,1,1,1,TRUE\n",
      "33771,38173,1,?,1,?,1,1,1,1,1,TRUE\n",
      "17029,37657,1,?,1,?,1,1,1,1,1,TRUE\n",
      "94492,94494,1,?,1,?,1,1,1,1,1,TRUE\n",
      "2318,14303,1,?,1,?,1,1,1,1,1,TRUE\n",
      "2481,10949,1,?,1,?,1,1,1,1,1,TRUE\n",
      "25247,25711,1,?,1,?,1,1,1,1,1,TRUE\n"
     ]
    }
   ],
   "source": [
    "head.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"\n"
     ]
    }
   ],
   "source": [
    "def isHeader(line:String) = line.contains(\"id_1\")\n",
    "head.filter(isHeader).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37671,78628,1,?,1,?,1,1,1,1,1,TRUE\n",
      "57064,74984,1,1,1,?,1,1,1,1,1,TRUE\n",
      "3149,8328,1,?,1,?,1,1,1,1,1,TRUE\n",
      "33771,38173,1,?,1,?,1,1,1,1,1,TRUE\n",
      "17029,37657,1,?,1,?,1,1,1,1,1,TRUE\n",
      "94492,94494,1,?,1,?,1,1,1,1,1,TRUE\n",
      "2318,14303,1,?,1,?,1,1,1,1,1,TRUE\n",
      "2481,10949,1,?,1,?,1,1,1,1,1,TRUE\n",
      "25247,25711,1,?,1,?,1,1,1,1,1,TRUE\n"
     ]
    }
   ],
   "source": [
    "head.filter(!isHeader(_)).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val noheader = rawblocks.filter(!isHeader(_))\n",
    "noheader.first\n",
    "noheader.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noheader.filter(x=>x.contains(\"cmp_fname_c1\")).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17029\n",
      "37657\n",
      "1\n",
      "?\n",
      "1\n",
      "?\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "val line = head(5)\n",
    "val pieces = line.split(',')\n",
    "pieces.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17029 37657 true\n"
     ]
    }
   ],
   "source": [
    "val id1 = pieces(0).toInt\n",
    "val id2 = pieces(1).toInt\n",
    "val matched = pieces(11).toBoolean\n",
    "println(id1+\" \"+id2+\" \"+matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.NumberFormatException\n",
       "Message: For input string: \"?\"\n",
       "StackTrace:   at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
       "  at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
       "  at java.lang.Double.parseDouble(Double.java:538)\n",
       "  at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284)\n",
       "  at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29)\n",
       "  at $anonfun$1.apply(<console>:42)\n",
       "  at $anonfun$1.apply(<console>:42)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawscores = pieces.slice(2,11)\n",
    "rawscores.map(s => s.toDouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "NaN\n",
      "1.0\n",
      "NaN\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def toDouble(s:String)={\n",
    "    if(\"?\".equals(s)) Double.NaN else s.toDouble\n",
    "}\n",
    "val scores = rawscores.map(toDouble)\n",
    "scores.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17029,37657,[D@6a2cb182,true)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseOld(line:String)={\n",
    "    val pieces = line.split(\",\")\n",
    "    val id1 = pieces(0).toInt\n",
    "    val id2 = pieces(1).toInt\n",
    "    val scores = pieces.slice(2,11).map(toDouble)\n",
    "    val matched = pieces(11).toBoolean\n",
    "    (id1, id2, scores, matched)\n",
    "}\n",
    "val tup = parseOld(line)\n",
    "tup.toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17029 37657 9 true\n"
     ]
    }
   ],
   "source": [
    "case class MatchData(id1:Int, id2:Int,\n",
    "                    scores:Array[Double], matched: Boolean)\n",
    "def parse(line:String)={\n",
    "    val pieces = line.split(\",\")\n",
    "    val id1 = pieces(0).toInt\n",
    "    val id2 = pieces(1).toInt\n",
    "    val scores = pieces.slice(2,11).map(toDouble)\n",
    "    val matched = pieces(11).toBoolean\n",
    "    MatchData(id1, id2, scores, matched)\n",
    "}\n",
    "val md = parse(line)\n",
    "println(md.id1+\" \"+md.id2+\" \"+md.scores.length+\" \"+md.matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mds = head.filter(!isHeader(_)).map(parse(_))\n",
    "mds.head\n",
    "mds.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsed = noheader.map(parse(_))\n",
    "parsed.cache\n",
    "parsed.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(true,9)\n"
     ]
    }
   ],
   "source": [
    "val grouped = mds.groupBy(md=>md.matched)\n",
    "grouped.mapValues(x=>x.size).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val allgrouped = parsed.groupBy(x=>x.matched)\n",
    "//allgrouped.mapValues(_.size).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val matchCount = parsed.map(x=>x.matched).countByValue()\n",
    "matchCount.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(true,20931)\n",
      "(false,5728201)\n"
     ]
    }
   ],
   "source": [
    "matchCount.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val matchCountSeq = matchCount.toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(false,5728201)\n",
      "(true,20931)\n"
     ]
    }
   ],
   "source": [
    "matchCountSeq.sortBy(_._1).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(true,20931)\n",
      "(false,5728201)\n"
     ]
    }
   ],
   "source": [
    "matchCountSeq.sortBy(_._2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(false,5728201)\n",
      "(true,20931)\n"
     ]
    }
   ],
   "source": [
    "matchCountSeq.sortBy(_._2).reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 5749132, mean: NaN, stdev: NaN, max: NaN, min: NaN)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.map(x=>x.scores(0)).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 5748125, mean: 0.712902, stdev: 0.388758, max: 1.000000, min: 0.000000)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.lang.Double.isNaN\n",
    "parsed.map(x=>x.scores(0)).filter(!isNaN(_)).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat[0]:(count: 5748125, mean: 0.712902, stdev: 0.388758, max: 1.000000, min: 0.000000)\n",
      "stat[1]:(count: 103698, mean: 0.900018, stdev: 0.271316, max: 1.000000, min: 0.000000)\n",
      "stat[2]:(count: 5749132, mean: 0.315628, stdev: 0.334234, max: 1.000000, min: 0.000000)\n",
      "stat[3]:(count: 2464, mean: 0.318413, stdev: 0.368492, max: 1.000000, min: 0.000000)\n",
      "stat[4]:(count: 5749132, mean: 0.955001, stdev: 0.207301, max: 1.000000, min: 0.000000)\n",
      "stat[5]:(count: 5748337, mean: 0.224465, stdev: 0.417230, max: 1.000000, min: 0.000000)\n",
      "stat[6]:(count: 5748337, mean: 0.488855, stdev: 0.499876, max: 1.000000, min: 0.000000)\n",
      "stat[7]:(count: 5748337, mean: 0.222749, stdev: 0.416091, max: 1.000000, min: 0.000000)\n",
      "stat[8]:(count: 5736289, mean: 0.005529, stdev: 0.074149, max: 1.000000, min: 0.000000)\n"
     ]
    }
   ],
   "source": [
    "val stats = (0 until 9).map(i=>{\n",
    "    println(\"stat[\" + i + \"]:\" +parsed.map(x=>x.scores(i)).filter(!isNaN(_)).stats())\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stats: (count: 1, mean: 17.290000, stdev: 0.000000, max: 17.290000, min: 17.290000) NaN: 0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import org.apache.spark.util.StatCounter\n",
    "class NAStatCounter extends Serializable{\n",
    "    import org.apache.spark.util.StatCounter\n",
    "    val stats = new StatCounter\n",
    "    var missing: Long = 0\n",
    "\n",
    "    def add(x:Double):NAStatCounter ={\n",
    "        if(java.lang.Double.isNaN(x)){\n",
    "            missing += 1\n",
    "        }else{\n",
    "            stats.merge(x)\n",
    "        }\n",
    "        this\n",
    "    }\n",
    "    \n",
    "    def merge(other: NAStatCounter): NAStatCounter = {\n",
    "        stats.merge(other.stats)\n",
    "        missing += other.missing\n",
    "        this\n",
    "    }\n",
    "    \n",
    "    override def toString = {\n",
    "        \"stats: \" + stats.toString + \" NaN: \" + missing\n",
    "    }\n",
    "}\n",
    "\n",
    "object NAStatCounter extends Serializable{\n",
    "    def apply(x: Double) = new NAStatCounter().add(x)\n",
    "}\n",
    "\n",
    "val nastats = NAStatCounter(17.29)\n",
    "nastats.toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stats: (count: 2, mean: 6.050000, stdev: 3.950000, max: 10.000000, min: 2.100000) NaN: 0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nas1 = NAStatCounter(10.0)\n",
    "nas1.add(2.1)\n",
    "nas1.toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stats: (count: 2, mean: 6.050000, stdev: 3.950000, max: 10.000000, min: 2.100000) NaN: 1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nas2 = NAStatCounter(Double.NaN)\n",
    "nas1.merge(nas2)\n",
    "nas1.toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: (count: 1, mean: 1.000000, stdev: 0.000000, max: 1.000000, min: 1.000000) NaN: 0\n",
      "stats: (count: 0, mean: 0.000000, stdev: NaN, max: -Infinity, min: Infinity) NaN: 1\n",
      "stats: (count: 1, mean: 17.290000, stdev: 0.000000, max: 17.290000, min: 17.290000) NaN: 0\n"
     ]
    }
   ],
   "source": [
    "val arr = Array(1.0, Double.NaN, 17.29)\n",
    "val nas = arr.map(d => NAStatCounter(d))\n",
    "nas.foreach(e=>println(e.toString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: (count: 1, mean: 1.000000, stdev: 0.000000, max: 1.000000, min: 1.000000) NaN: 1\n",
      "stats: (count: 1, mean: 2.000000, stdev: 0.000000, max: 2.000000, min: 2.000000) NaN: 1\n"
     ]
    }
   ],
   "source": [
    "val nas1 = Array(1.0, Double.NaN).map(NAStatCounter(_))\n",
    "val nas2 = Array(Double.NaN, 2.0).map(NAStatCounter(_))\n",
    "val merged = nas1.zip(nas2).map(p=> p._1.merge(p._2))\n",
    "merged.foreach(e=>println(e.toString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: (count: 1, mean: 1.000000, stdev: 0.000000, max: 1.000000, min: 1.000000) NaN: 1\n",
      "stats: (count: 1, mean: 2.000000, stdev: 0.000000, max: 2.000000, min: 2.000000) NaN: 1\n"
     ]
    }
   ],
   "source": [
    "val nas11 = Array(1.0, Double.NaN).map(NAStatCounter(_))\n",
    "val nas12 = Array(Double.NaN, 2.0).map(NAStatCounter(_))\n",
    "val merged1 = nas11.zip(nas12).map{case(a,b)=>a.merge(b)}\n",
    "merged1.foreach(e=>println(e.toString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: (count: 1, mean: 1.000000, stdev: 0.000000, max: 1.000000, min: 1.000000) NaN: 1\n",
      "stats: (count: 1, mean: 2.000000, stdev: 0.000000, max: 2.000000, min: 2.000000) NaN: 1\n"
     ]
    }
   ],
   "source": [
    "val nas21 = Array(1.0, Double.NaN).map(NAStatCounter(_))\n",
    "val nas22 = Array(Double.NaN, 2.0).map(NAStatCounter(_))\n",
    "val nas = List(nas21, nas22)\n",
    "val merged2 = nas.reduce((n1,n2)=>{\n",
    "    n1.zip(n2).map{case(a,b)=>a.merge(b)}\n",
    "})\n",
    "merged2.foreach(e=>println(e.toString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 364, localhost, executor driver): java.lang.NoClassDefFoundError: Could not initialize class $line89.$read$\n",
       "\tat $line360.$read$$iw.<init>(<console>:13)\n",
       "\tat $line360.$read.<init>(<console>:79)\n",
       "\tat $line360.$read$.<init>(<console>:83)\n",
       "\tat $line360.$read$.<clinit>(<console>)\n",
       "\tat $line361.$read$$iw$$iw$$iw$$iw$$iw$$iw$NAStatCounter$.apply(<console>:67)\n",
       "\tat $line393.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1$$anonfun$apply$1.apply(<console>:70)\n",
       "\tat $line393.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1$$anonfun$apply$1.apply(<console>:70)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofDouble.foreach(ArrayOps.scala:270)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat scala.collection.mutable.ArrayOps$ofDouble.map(ArrayOps.scala:270)\n",
       "\tat $line393.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:70)\n",
       "\tat $line393.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:69)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n",
       "\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat <init>(<console>:13)\n",
       "\tat <init>(<console>:79)\n",
       "\tat .<init>(<console>:83)\n",
       "\tat .<clinit>(<console>)\n",
       "\tat NAStatCounter$.apply(<console>:67)\n",
       "\tat $anonfun$1$$anonfun$apply$1.apply(<console>:70)\n",
       "\tat $anonfun$1$$anonfun$apply$1.apply(<console>:70)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofDouble.foreach(ArrayOps.scala:270)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat scala.collection.mutable.ArrayOps$ofDouble.map(ArrayOps.scala:270)\n",
       "\tat $anonfun$1.apply(<console>:70)\n",
       "\tat $anonfun$1.apply(<console>:69)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n",
       "\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n",
       "  ... 42 elided\n",
       "Caused by: java.lang.NoClassDefFoundError: Could not initialize class "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nasRDD = parsed.map(md => {\n",
    "    md.scores.map(NAStatCounter(_))\n",
    "})\n",
    "val reduced = nasRDD.reduce((n1,n2)=>{\n",
    "    n1.zip(n2).map{case(a,b)=>a.merge(b)}\n",
    "})\n",
    "reduced.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark 2.1.0(scala 2.12.1)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
